{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query-Aware Dynamic Chunking (LlamaIndex, Isolated)\n",
        "\n",
        "- `method_id`: `1.4_query_aware_dynamic_chunking`\n",
        "- `category`: `Chunk-Based Indexing`\n",
        "- `goal`: Query-time chunk assembly around intent-bearing spans.\n",
        "\n",
        "This notebook is fully self-contained. It includes the full experiment implementation and does not import project-local framework modules at runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell in a fresh notebook environment.\n",
        "%pip install -q llama-index llama-index-embeddings-huggingface llama-index-retrievers-bm25 numpy pandas scikit-learn networkx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if not (repo_root / \"data\").exists():\n",
        "    repo_root = repo_root.parent\n",
        "\n",
        "corpus_path = repo_root / \"data\" / \"sample_corpus.jsonl\"\n",
        "queries_path = repo_root / \"data\" / \"sample_queries.jsonl\"\n",
        "results_dir = repo_root / \"results\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import date, datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from llama_index.core import Document, Settings, StorageContext, VectorStoreIndex\n",
        "from llama_index.core.embeddings import MockEmbedding\n",
        "from llama_index.core.node_parser import (\n",
        "    HierarchicalNodeParser,\n",
        "    SemanticSplitterNodeParser,\n",
        "    SentenceSplitter,\n",
        "    SentenceWindowNodeParser,\n",
        "    TokenTextSplitter,\n",
        "    get_leaf_nodes,\n",
        ")\n",
        "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
        "\n",
        "try:\n",
        "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "except Exception:\n",
        "    HuggingFaceEmbedding = None\n",
        "\n",
        "try:\n",
        "    from llama_index.retrievers.bm25 import BM25Retriever\n",
        "except Exception:\n",
        "    BM25Retriever = None\n",
        "\n",
        "try:\n",
        "    import networkx as nx\n",
        "except Exception:\n",
        "    nx = None\n",
        "\n",
        "\n",
        "TOKEN_RE = re.compile(r\"[a-zA-Z0-9']+\")\n",
        "ENTITY_RE = re.compile(r\"\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2}\\b\")\n",
        "SCENE_RE = re.compile(r\"\\[SCENE\\s*:\\s*([^\\]]+)\\]\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MethodSpec:\n",
        "    method_id: str\n",
        "    category: str\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScoredNode:\n",
        "    node_id: str\n",
        "    doc_id: str\n",
        "    score: float\n",
        "    text: str\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "METHOD_SPECS: List[MethodSpec] = [\n",
        "    MethodSpec(\"1.1_fixed_length_chunking\", \"Chunk-Based Indexing\", \"Fixed-Length Chunking\", \"Uniform token windows with predictable memory usage.\"),\n",
        "    MethodSpec(\"1.2_sliding_window_overlap\", \"Chunk-Based Indexing\", \"Sliding Window with Overlap\", \"Overlapping windows to preserve context transitions.\"),\n",
        "    MethodSpec(\"1.3_adaptive_semantic_chunking\", \"Chunk-Based Indexing\", \"Adaptive Semantic Chunking\", \"Chunks generated using semantic coherence boundaries.\"),\n",
        "    MethodSpec(\"1.4_query_aware_dynamic_chunking\", \"Chunk-Based Indexing\", \"Query-Aware Dynamic Chunking\", \"Query-time chunk assembly around intent-bearing spans.\"),\n",
        "    MethodSpec(\"1.5_small_to_big_hierarchical_chunking\", \"Chunk-Based Indexing\", \"Small-to-Big Hierarchical Chunking\", \"Retrieve fine chunks and respond with parent context.\"),\n",
        "    MethodSpec(\"1.6_sentence_level_indexing\", \"Chunk-Based Indexing\", \"Sentence-Level Indexing\", \"Sentence-granular retrieval with local context links.\"),\n",
        "    MethodSpec(\"2.1_flat_dense_index\", \"Vector-Based Indexing\", \"Flat Dense Index\", \"Exhaustive dense retrieval across all embeddings.\"),\n",
        "    MethodSpec(\"2.2_approximate_nearest_neighbor_index\", \"Vector-Based Indexing\", \"Approximate Nearest Neighbor Index\", \"Cluster-pruned candidate search for faster retrieval.\"),\n",
        "    MethodSpec(\"2.3_quantized_vector_index\", \"Vector-Based Indexing\", \"Quantized Vector Index\", \"Compressed vectors for reduced memory footprint.\"),\n",
        "    MethodSpec(\"2.4_hybrid_dense_partition_index\", \"Vector-Based Indexing\", \"Hybrid Dense Partition Index\", \"Partition routing followed by dense rerank.\"),\n",
        "    MethodSpec(\"3.1_inverted_index\", \"Sparse and Lexical Indexing\", \"Inverted Index\", \"Term-to-postings lexical retrieval.\"),\n",
        "    MethodSpec(\"3.2_sparse_dense_fusion_index\", \"Sparse and Lexical Indexing\", \"Sparse-Dense Fusion Index\", \"Weighted fusion of lexical and dense scores.\"),\n",
        "    MethodSpec(\"3.3_weighted_term_index\", \"Sparse and Lexical Indexing\", \"Weighted Term Index\", \"BM25-style weighted term relevance ranking.\"),\n",
        "    MethodSpec(\"4.1_tree_structured_index\", \"Hierarchical Indexing\", \"Tree-Structured Index\", \"Parent-child index with section-aware retrieval.\"),\n",
        "    MethodSpec(\"4.2_summary_augmented_index\", \"Hierarchical Indexing\", \"Summary-Augmented Index\", \"Summary-first routing to full chunks.\"),\n",
        "    MethodSpec(\"4.3_structural_aware_index\", \"Hierarchical Indexing\", \"Structural-Aware Index\", \"Chunking aligned with headings and structural blocks.\"),\n",
        "    MethodSpec(\"5.1_knowledge_graph_index\", \"Graph-Based Indexing\", \"Knowledge Graph Index\", \"Entity-passage graph with relation-aware retrieval.\"),\n",
        "    MethodSpec(\"5.2_entity_centric_index\", \"Graph-Based Indexing\", \"Entity-Centric Index\", \"Entity-first retrieval and passage linkage.\"),\n",
        "    MethodSpec(\"5.3_semantic_similarity_graph\", \"Graph-Based Indexing\", \"Semantic Similarity Graph\", \"kNN graph over semantic embeddings.\"),\n",
        "    MethodSpec(\"5.4_multi_hop_retrieval_graph\", \"Graph-Based Indexing\", \"Multi-Hop Retrieval Graph\", \"Graph traversal for multi-hop evidence gathering.\"),\n",
        "    MethodSpec(\"6.1_topic_partition_index\", \"Topic and Cluster-Based Indexing\", \"Topic Partition Index\", \"Query routing into thematic clusters.\"),\n",
        "    MethodSpec(\"6.2_semantic_hashing_index\", \"Topic and Cluster-Based Indexing\", \"Semantic Hashing Index\", \"Binary hash retrieval with hamming filtering.\"),\n",
        "    MethodSpec(\"7.1_time_decayed_index\", \"Temporal and Streaming Indexing\", \"Time-Decayed Index\", \"Recency-aware ranking with temporal decay.\"),\n",
        "    MethodSpec(\"7.2_sliding_temporal_window_index\", \"Temporal and Streaming Indexing\", \"Sliding Temporal Window Index\", \"Separate recent and historical retrieval windows.\"),\n",
        "    MethodSpec(\"7.3_real_time_streaming_index\", \"Temporal and Streaming Indexing\", \"Real-Time Streaming Index\", \"Incremental insertion without full rebuild.\"),\n",
        "    MethodSpec(\"8.1_trust_weighted_index\", \"Robust and Adversarial-Aware Indexing\", \"Trust-Weighted Index\", \"Provenance-weighted ranking.\"),\n",
        "    MethodSpec(\"8.2_outlier_aware_vector_index\", \"Robust and Adversarial-Aware Indexing\", \"Outlier-Aware Vector Index\", \"Outlier filtering at insertion/search.\"),\n",
        "    MethodSpec(\"8.3_drift_aware_index\", \"Robust and Adversarial-Aware Indexing\", \"Drift-Aware Index\", \"Distribution-shift-aware ranking controls.\"),\n",
        "    MethodSpec(\"9.1_distributional_embedding_index\", \"Probabilistic and Uncertainty-Aware Indexing\", \"Distributional Embedding Index\", \"Mean/variance vector scoring.\"),\n",
        "    MethodSpec(\"9.2_confidence_propagating_index\", \"Probabilistic and Uncertainty-Aware Indexing\", \"Confidence-Propagating Index\", \"Uncertainty-adjusted confidence ranking.\"),\n",
        "    MethodSpec(\"10.1_joint_embedding_index\", \"Multi-Modal Indexing\", \"Joint Embedding Index\", \"Shared embedding space for mixed modalities.\"),\n",
        "    MethodSpec(\"10.2_cross_modal_linked_index\", \"Multi-Modal Indexing\", \"Cross-Modal Linked Index\", \"Modality-specific nodes linked by alignment edges.\"),\n",
        "    MethodSpec(\"10.3_scene_aware_media_index\", \"Multi-Modal Indexing\", \"Scene-Aware Media Index\", \"Scene segmentation with OCR/transcript metadata.\"),\n",
        "    MethodSpec(\"11.1_self_tuning_index\", \"Adaptive and Continual Indexing\", \"Self-Tuning Index\", \"Automatic parameter adaptation to corpus characteristics.\"),\n",
        "    MethodSpec(\"11.2_feedback_driven_index\", \"Adaptive and Continual Indexing\", \"Feedback-Driven Index\", \"Ranking updates from interaction feedback.\"),\n",
        "    MethodSpec(\"11.3_continual_re_embedding_index\", \"Adaptive and Continual Indexing\", \"Continual Re-Embedding Index\", \"Periodic re-embedding for model upgrades.\"),\n",
        "    MethodSpec(\"12.1_causal_graph_index\", \"Reasoning-Ready Indexing\", \"Causal Graph Index\", \"Cause-effect relation graph retrieval.\"),\n",
        "    MethodSpec(\"12.2_explanation_aware_index\", \"Reasoning-Ready Indexing\", \"Explanation-Aware Index\", \"Reasoning chain annotations in ranking.\"),\n",
        "    MethodSpec(\"12.3_plan_oriented_index\", \"Reasoning-Ready Indexing\", \"Plan-Oriented Index\", \"Action/procedure-aware content annotation.\"),\n",
        "]\n",
        "\n",
        "METHOD_SPEC_MAP: Dict[str, MethodSpec] = {m.method_id: m for m in METHOD_SPECS}\n",
        "\n",
        "\n",
        "def method_slug(method_id: str) -> str:\n",
        "    return method_id.replace(\".\", \"_\")\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return TOKEN_RE.findall(text.lower())\n",
        "\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    pieces = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    return [p.strip() for p in pieces if p.strip()]\n",
        "\n",
        "\n",
        "def safe_float(value: Any, default: float = 0.0) -> float:\n",
        "    try:\n",
        "        return float(value)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "\n",
        "def parse_date(value: Optional[str]) -> Optional[date]:\n",
        "    if not value:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.strptime(value[:10], \"%Y-%m-%d\").date()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_scores(scores: np.ndarray) -> np.ndarray:\n",
        "    if len(scores) == 0:\n",
        "        return scores\n",
        "    valid = np.isfinite(scores)\n",
        "    if not valid.any():\n",
        "        return np.zeros_like(scores)\n",
        "    v = scores.copy()\n",
        "    min_v = np.min(v[valid])\n",
        "    max_v = np.max(v[valid])\n",
        "    if math.isclose(min_v, max_v):\n",
        "        out = np.zeros_like(v)\n",
        "        out[valid] = 1.0\n",
        "        return out\n",
        "    out = np.zeros_like(v)\n",
        "    out[valid] = (v[valid] - min_v) / (max_v - min_v)\n",
        "    return out\n",
        "\n",
        "\n",
        "def node_text(node: BaseNode) -> str:\n",
        "    try:\n",
        "        return node.get_content(metadata_mode=\"none\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            return str(node.text)\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "def lexical_diversity(text: str) -> float:\n",
        "    tokens = tokenize(text)\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    return len(set(tokens)) / len(tokens)\n",
        "\n",
        "\n",
        "def extract_entities(text: str) -> List[str]:\n",
        "    entities = [e.strip() for e in ENTITY_RE.findall(text)]\n",
        "    return [e for e in entities if len(e) > 2]\n",
        "\n",
        "\n",
        "def extract_scene_chunks(text: str) -> List[Tuple[str, str]]:\n",
        "    matches = list(SCENE_RE.finditer(text))\n",
        "    if not matches:\n",
        "        return [(\"full\", text)]\n",
        "    chunks: List[Tuple[str, str]] = []\n",
        "    for i, match in enumerate(matches):\n",
        "        start = match.end()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "        scene_name = match.group(1).strip().lower().replace(\" \", \"_\")\n",
        "        body = text[start:end].strip()\n",
        "        if body:\n",
        "            chunks.append((scene_name, body))\n",
        "    return chunks or [(\"full\", text)]\n",
        "\n",
        "\n",
        "def chunk_structural(text: str) -> List[Tuple[str, int, str]]:\n",
        "    lines = text.splitlines() or [text]\n",
        "    heading = \"root\"\n",
        "    depth = 0\n",
        "    buf: List[str] = []\n",
        "    out: List[Tuple[str, int, str]] = []\n",
        "\n",
        "    def flush() -> None:\n",
        "        if buf:\n",
        "            block = \"\\n\".join(buf).strip()\n",
        "            if block:\n",
        "                out.append((heading, depth, block))\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if stripped.startswith(\"#\"):\n",
        "            flush()\n",
        "            buf = []\n",
        "            hashes = len(stripped) - len(stripped.lstrip(\"#\"))\n",
        "            heading = stripped[hashes:].strip() or \"section\"\n",
        "            depth = hashes\n",
        "        else:\n",
        "            buf.append(line)\n",
        "    flush()\n",
        "    if not out:\n",
        "        out.append((\"root\", 0, text))\n",
        "    return out\n",
        "\n",
        "\n",
        "def _configure_embed_models() -> Tuple[Any, Any, str]:\n",
        "    preferred = os.getenv(\"LLAMAINDEX_EMBED_MODEL\", \"BAAI/bge-small-en-v1.5\")\n",
        "    force_mock = os.getenv(\"LLAMAINDEX_FORCE_MOCK\", \"0\") == \"1\"\n",
        "    embed_model = None\n",
        "    v2_model = None\n",
        "    source = \"mock\"\n",
        "\n",
        "    if not force_mock and HuggingFaceEmbedding is not None:\n",
        "        try:\n",
        "            embed_model = HuggingFaceEmbedding(model_name=preferred)\n",
        "            source = preferred\n",
        "        except Exception:\n",
        "            embed_model = None\n",
        "\n",
        "        if embed_model is not None:\n",
        "            try:\n",
        "                v2_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "            except Exception:\n",
        "                v2_model = None\n",
        "\n",
        "    if embed_model is None:\n",
        "        embed_model = MockEmbedding(embed_dim=384)\n",
        "        source = \"mock\"\n",
        "\n",
        "    if v2_model is None:\n",
        "        v2_model = embed_model\n",
        "\n",
        "    Settings.embed_model = embed_model\n",
        "    return embed_model, v2_model, source\n",
        "\n",
        "\n",
        "def default_corpus_records() -> List[Dict[str, Any]]:\n",
        "    return [\n",
        "        {\n",
        "            \"id\": \"doc_001\",\n",
        "            \"title\": \"Industrial Revolution Grade 10\",\n",
        "            \"text\": \"# Industrial Revolution\\n## Grade 10 Overview\\nThe Industrial Revolution began in Britain and spread through Europe. Factories expanded textile production and urban migration increased. Steam engines improved transport and manufacturing.\",\n",
        "            \"timestamp\": \"2025-01-12\",\n",
        "            \"trust_score\": 0.90,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_ir_text\",\n",
        "            \"grade_level\": \"grade_10\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_002\",\n",
        "            \"title\": \"Industrial Revolution Grade 12\",\n",
        "            \"text\": \"# Industrial Revolution\\n## Grade 12 Analysis\\nAt Grade 12 depth, analysis includes labor exploitation, global supply chains, and colonial extraction. Mechanization increased productivity but widened inequality. This perspective contrasts with introductory summaries.\",\n",
        "            \"timestamp\": \"2025-01-15\",\n",
        "            \"trust_score\": 0.92,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_ir_text\",\n",
        "            \"grade_level\": \"grade_12\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_003\",\n",
        "            \"title\": \"French Revolution Causes\",\n",
        "            \"text\": \"# French Revolution\\nThe Revolution occurred because fiscal crisis, social inequality, and political distrust converged. Bread prices rose, tax burdens were uneven, and institutional legitimacy collapsed.\",\n",
        "            \"timestamp\": \"2024-11-03\",\n",
        "            \"trust_score\": 0.95,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_fr\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_004\",\n",
        "            \"title\": \"World War I Timeline\",\n",
        "            \"text\": \"# World War I\\n## Timeline\\n1914 mobilization accelerated alliances. 1916 attrition battles reshaped strategy. 1918 armistice ended frontline fighting.\",\n",
        "            \"timestamp\": \"2024-09-01\",\n",
        "            \"trust_score\": 0.88,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_ww1\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_005\",\n",
        "            \"title\": \"Civil Rights Movement\",\n",
        "            \"text\": \"# Civil Rights Movement\\nThe movement is significant because coordinated legal strategy and mass mobilization challenged segregation. Therefore, federal legislation shifted enforcement and voting protections.\",\n",
        "            \"timestamp\": \"2025-02-10\",\n",
        "            \"trust_score\": 0.93,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_cr\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_006\",\n",
        "            \"title\": \"Renewable Energy Policy 2025\",\n",
        "            \"text\": \"# Energy Update\\nRecent policy expanded grid storage incentives and offshore wind permitting. The newest update in 2025 emphasizes rapid interconnection approvals.\",\n",
        "            \"timestamp\": \"2025-12-02\",\n",
        "            \"trust_score\": 0.89,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_energy\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_007\",\n",
        "            \"title\": \"Cybersecurity Rumor Thread\",\n",
        "            \"text\": \"Unverified post claims all banks were breached overnight. No evidence is provided and details conflict across reposts.\",\n",
        "            \"timestamp\": \"2025-12-10\",\n",
        "            \"trust_score\": 0.25,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_cyber\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_008\",\n",
        "            \"title\": \"Cybersecurity Incident Bulletin\",\n",
        "            \"text\": \"# Incident Bulletin\\nA regional outage occurred due to a misconfigured firewall policy. Forensics found no confirmed data exfiltration. Recommended controls include staged rollout and immutable logging.\",\n",
        "            \"timestamp\": \"2025-12-11\",\n",
        "            \"trust_score\": 0.97,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_cyber\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_009\",\n",
        "            \"title\": \"Steam Engine Diagram OCR\",\n",
        "            \"text\": \"[OCR] Watt condenser reduces energy loss. Boiler pressure must be monitored for safety.\",\n",
        "            \"timestamp\": \"2025-03-08\",\n",
        "            \"trust_score\": 0.86,\n",
        "            \"modality\": \"image\",\n",
        "            \"asset_id\": \"asset_steam_1\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_010\",\n",
        "            \"title\": \"Steam Engine Lecture Audio\",\n",
        "            \"text\": \"[TRANSCRIPT] Audio lecture explains that steam engines convert thermal energy into mechanical work and transformed factory throughput.\",\n",
        "            \"timestamp\": \"2025-03-08\",\n",
        "            \"trust_score\": 0.88,\n",
        "            \"modality\": \"audio\",\n",
        "            \"asset_id\": \"asset_steam_1\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_011\",\n",
        "            \"title\": \"Factory Conditions Documentary\",\n",
        "            \"text\": \"[SCENE: opening] Narrator introduces urban growth. [SCENE: factory_floor] Workers describe long shifts and ventilation hazards. [SCENE: reform] Later labor laws reduce child labor.\",\n",
        "            \"timestamp\": \"2025-04-01\",\n",
        "            \"trust_score\": 0.90,\n",
        "            \"modality\": \"video\",\n",
        "            \"asset_id\": \"asset_factory_video\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_012\",\n",
        "            \"title\": \"Archiving Procedure\",\n",
        "            \"text\": \"Step 1: Inventory materials and label provenance. Step 2: Scan with checksum validation. Step 3: Store originals in humidity-controlled cabinets. Then publish a searchable catalog.\",\n",
        "            \"timestamp\": \"2025-07-21\",\n",
        "            \"trust_score\": 0.94,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_archive\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_013\",\n",
        "            \"title\": \"Climate Transition Brief\",\n",
        "            \"text\": \"# Climate Brief\\nGrid modernization and heat-pump adoption accelerated in late 2025. Financing barriers remain but deployment rates improved compared with 2024.\",\n",
        "            \"timestamp\": \"2025-11-19\",\n",
        "            \"trust_score\": 0.91,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_energy\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_014\",\n",
        "            \"title\": \"Adversarial Keyword Stuffing\",\n",
        "            \"text\": \"industrial revolution industrial revolution industrial revolution miracle cure click now now now. unrelated payload and random symbols.\",\n",
        "            \"timestamp\": \"2025-10-10\",\n",
        "            \"trust_score\": 0.10,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_noise\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_015\",\n",
        "            \"title\": \"Supply Chain Delay Analysis\",\n",
        "            \"text\": \"Delays happened because port congestion and component shortages led to cascading schedule slips. Therefore procurement teams shifted to dual sourcing.\",\n",
        "            \"timestamp\": \"2025-08-14\",\n",
        "            \"trust_score\": 0.87,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_supply\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_016\",\n",
        "            \"title\": \"Explanation Skills Guide\",\n",
        "            \"text\": \"To explain a concept clearly, start with prior knowledge, then connect new evidence, and finally verify understanding. This means explanations should include reasoned transitions.\",\n",
        "            \"timestamp\": \"2025-05-29\",\n",
        "            \"trust_score\": 0.85,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_pedagogy\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_017\",\n",
        "            \"title\": \"Historical Data Archive 1998\",\n",
        "            \"text\": \"# Historical Archive\\nA 1998 index process used manual card catalogs and weekly reconciliation. Digitization later replaced physical lookup constraints.\",\n",
        "            \"timestamp\": \"2023-06-12\",\n",
        "            \"trust_score\": 0.82,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_archive\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_018\",\n",
        "            \"title\": \"AI Ethics Topic Note\",\n",
        "            \"text\": \"AI ethics includes fairness audits, model transparency, and accountability controls. Governance frameworks align deployment with policy obligations.\",\n",
        "            \"timestamp\": \"2025-09-12\",\n",
        "            \"trust_score\": 0.90,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_ai_ethics\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_019\",\n",
        "            \"title\": \"Medieval Trade Grade 10\",\n",
        "            \"text\": \"# Medieval Trade\\n## Grade 10\\nTrade routes connected towns through fairs and merchant guilds. Students focus on basic exchange flows and goods movement.\",\n",
        "            \"timestamp\": \"2025-01-20\",\n",
        "            \"trust_score\": 0.88,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_trade\",\n",
        "            \"grade_level\": \"grade_10\",\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"doc_020\",\n",
        "            \"title\": \"Medieval Trade Grade 12\",\n",
        "            \"text\": \"# Medieval Trade\\n## Grade 12\\nAdvanced treatment examines credit instruments, maritime insurance, and institutional effects on early capitalism.\",\n",
        "            \"timestamp\": \"2025-01-22\",\n",
        "            \"trust_score\": 0.89,\n",
        "            \"modality\": \"text\",\n",
        "            \"asset_id\": \"asset_trade\",\n",
        "            \"grade_level\": \"grade_12\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "def default_query_records() -> List[Dict[str, Any]]:\n",
        "    return [\n",
        "        {\n",
        "            \"query\": \"Compare the treatment of the Industrial Revolution in Grade 10 vs Grade 12.\",\n",
        "            \"relevant_ids\": [\"doc_001\", \"doc_002\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"What caused the French Revolution?\",\n",
        "            \"relevant_ids\": [\"doc_003\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Explain why the Civil Rights Movement mattered.\",\n",
        "            \"relevant_ids\": [\"doc_005\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"What are the latest renewable energy policy updates?\",\n",
        "            \"relevant_ids\": [\"doc_006\", \"doc_013\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Find a trustworthy source about the recent cybersecurity incident.\",\n",
        "            \"relevant_ids\": [\"doc_008\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"What evidence exists across image and audio about steam engines?\",\n",
        "            \"relevant_ids\": [\"doc_009\", \"doc_010\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Which scene discusses factory floor conditions?\",\n",
        "            \"relevant_ids\": [\"doc_011\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Give steps to archive historical documents.\",\n",
        "            \"relevant_ids\": [\"doc_012\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Why did supply chain delays happen?\",\n",
        "            \"relevant_ids\": [\"doc_015\"],\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Find Grade 12 material on medieval trade.\",\n",
        "            \"relevant_ids\": [\"doc_020\"],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "        for line in handle:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def load_corpus(corpus_path: Optional[str | Path] = None) -> List[Dict[str, Any]]:\n",
        "    if corpus_path is None:\n",
        "        return default_corpus_records()\n",
        "    path = Path(corpus_path)\n",
        "    if not path.exists():\n",
        "        return default_corpus_records()\n",
        "    return load_jsonl(path)\n",
        "\n",
        "\n",
        "def load_queries(query_path: Optional[str | Path] = None) -> List[Dict[str, Any]]:\n",
        "    if query_path is None:\n",
        "        return default_query_records()\n",
        "    path = Path(query_path)\n",
        "    if not path.exists():\n",
        "        return default_query_records()\n",
        "    return load_jsonl(path)\n",
        "\n",
        "\n",
        "class LlamaIndexMethodRunner:\n",
        "    _CAUSAL_QUERY_MARKERS = (\"because\", \"cause\", \"caused\", \"led to\", \"why\")\n",
        "    _EXPLANATION_QUERY_MARKERS = (\"why\", \"explain\", \"reason\", \"because\")\n",
        "    _EXPLANATION_TEXT_MARKERS = (\"because\", \"therefore\", \"this means\", \"as a result\")\n",
        "    _PLAN_QUERY_MARKERS = (\"how\", \"steps\", \"procedure\", \"plan\")\n",
        "    _PLAN_TEXT_MARKERS = (\"step 1\", \"step 2\", \"first\", \"then\", \"procedure\")\n",
        "\n",
        "    def __init__(self, method_id: str) -> None:\n",
        "        if method_id not in METHOD_SPEC_MAP:\n",
        "            raise ValueError(f\"Unknown method_id: {method_id}\")\n",
        "\n",
        "        self.spec = METHOD_SPEC_MAP[method_id]\n",
        "        self.method_id = method_id\n",
        "        self.documents: List[Dict[str, Any]] = []\n",
        "\n",
        "        self.storage_context: Optional[StorageContext] = None\n",
        "        self.all_nodes: List[BaseNode] = []\n",
        "        self.index_nodes: List[BaseNode] = []\n",
        "        self.node_lookup: Dict[str, BaseNode] = {}\n",
        "        self.node_id_to_idx: Dict[str, int] = {}\n",
        "\n",
        "        self.vector_index: Optional[VectorStoreIndex] = None\n",
        "        self.dense_retriever: Any = None\n",
        "        self.bm25_retriever: Any = None\n",
        "\n",
        "        self.embed_model: Any = None\n",
        "        self.embed_model_v2: Any = None\n",
        "        self.embed_source: str = \"\"\n",
        "\n",
        "        self.node_embeddings: Optional[np.ndarray] = None\n",
        "        self.node_embeddings_v2: Optional[np.ndarray] = None\n",
        "\n",
        "        self.inverted: Dict[str, set[int]] = defaultdict(set)\n",
        "        self.term_freqs: List[Counter[str]] = []\n",
        "\n",
        "        self.cluster_labels: Optional[np.ndarray] = None\n",
        "        self.cluster_centers: Optional[np.ndarray] = None\n",
        "        self.hash_planes: Optional[np.ndarray] = None\n",
        "        self.hash_codes: Optional[np.ndarray] = None\n",
        "\n",
        "        self.summaries: List[str] = []\n",
        "        self.summary_embeddings: Optional[np.ndarray] = None\n",
        "\n",
        "        self.entity_index: Dict[str, set[int]] = defaultdict(set)\n",
        "        self.entity_graph: Any = None\n",
        "        self.semantic_graph: Dict[int, set[int]] = defaultdict(set)\n",
        "        self.causal_graph: Dict[int, set[int]] = defaultdict(set)\n",
        "\n",
        "        self.parent_map: Dict[str, str] = {}\n",
        "\n",
        "        self.trust_scores: Optional[np.ndarray] = None\n",
        "        self.age_days: Optional[np.ndarray] = None\n",
        "        self.variance_scores: Optional[np.ndarray] = None\n",
        "        self.confidence_scores: Optional[np.ndarray] = None\n",
        "        self.outlier_mask: Optional[np.ndarray] = None\n",
        "        self.drift_weights: Optional[np.ndarray] = None\n",
        "\n",
        "        self.feedback_bias: Dict[str, float] = defaultdict(float)\n",
        "        self.runtime_config: Dict[str, Any] = {}\n",
        "\n",
        "        self.candidate_k = 24\n",
        "\n",
        "    def _to_documents(self, records: Sequence[Dict[str, Any]]) -> List[Document]:\n",
        "        docs: List[Document] = []\n",
        "        for row in records:\n",
        "            doc_id = str(row.get(\"id\", f\"doc_{len(docs)}\"))\n",
        "            text = str(row.get(\"text\", \"\")).strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            modality = str(row.get(\"modality\", \"text\"))\n",
        "            if self.method_id in {\"10.1_joint_embedding_index\", \"10.2_cross_modal_linked_index\"}:\n",
        "                text = f\"[{modality}] {text}\"\n",
        "\n",
        "            metadata = {k: v for k, v in row.items() if k != \"text\"}\n",
        "            metadata[\"doc_id\"] = doc_id\n",
        "            docs.append(Document(text=text, metadata=metadata))\n",
        "        return docs\n",
        "\n",
        "    def _build_structural_nodes(self, docs: Sequence[Document]) -> List[BaseNode]:\n",
        "        nodes: List[BaseNode] = []\n",
        "        for doc in docs:\n",
        "            md = dict(doc.metadata or {})\n",
        "            for heading, depth, body in chunk_structural(str(doc.text_resource.text)):\n",
        "                metadata = dict(md)\n",
        "                metadata[\"heading\"] = heading\n",
        "                metadata[\"depth\"] = depth\n",
        "                nodes.append(TextNode(text=body, metadata=metadata))\n",
        "        return nodes\n",
        "\n",
        "    def _build_scene_nodes(self, docs: Sequence[Document]) -> List[BaseNode]:\n",
        "        nodes: List[BaseNode] = []\n",
        "        for doc in docs:\n",
        "            md = dict(doc.metadata or {})\n",
        "            text = str(doc.text_resource.text)\n",
        "            if str(md.get(\"modality\", \"text\")).lower() == \"video\":\n",
        "                for scene_name, scene_text in extract_scene_chunks(text):\n",
        "                    metadata = dict(md)\n",
        "                    metadata[\"scene\"] = scene_name\n",
        "                    nodes.append(TextNode(text=scene_text, metadata=metadata))\n",
        "            else:\n",
        "                nodes.append(TextNode(text=text, metadata=md))\n",
        "        return nodes\n",
        "\n",
        "    def _build_nodes(self, docs: Sequence[Document]) -> Tuple[List[BaseNode], List[BaseNode]]:\n",
        "        method = self.method_id\n",
        "\n",
        "        if method == \"11.1_self_tuning_index\":\n",
        "            lengths = [len(tokenize(str(doc.text_resource.text))) for doc in docs]\n",
        "            median_len = float(np.median(lengths)) if lengths else 120.0\n",
        "            if median_len < 120:\n",
        "                self.runtime_config[\"chunk_size\"] = 80\n",
        "            elif median_len < 220:\n",
        "                self.runtime_config[\"chunk_size\"] = 120\n",
        "            else:\n",
        "                self.runtime_config[\"chunk_size\"] = 200\n",
        "\n",
        "        if method == \"1.5_small_to_big_hierarchical_chunking\":\n",
        "            parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[512, 128], chunk_overlap=40)\n",
        "            all_nodes = parser.get_nodes_from_documents(docs)\n",
        "            leaf_nodes = get_leaf_nodes(all_nodes)\n",
        "            for node in leaf_nodes:\n",
        "                rel = (node.relationships or {}).get(NodeRelationship.PARENT)\n",
        "                if rel is not None:\n",
        "                    self.parent_map[node.node_id] = rel.node_id\n",
        "            return all_nodes, leaf_nodes\n",
        "\n",
        "        if method in {\"4.1_tree_structured_index\", \"4.2_summary_augmented_index\", \"4.3_structural_aware_index\"}:\n",
        "            nodes = self._build_structural_nodes(docs)\n",
        "            return nodes, nodes\n",
        "\n",
        "        if method == \"10.3_scene_aware_media_index\":\n",
        "            nodes = self._build_scene_nodes(docs)\n",
        "            return nodes, nodes\n",
        "\n",
        "        if method == \"1.3_adaptive_semantic_chunking\":\n",
        "            try:\n",
        "                parser = SemanticSplitterNodeParser.from_defaults(\n",
        "                    embed_model=Settings.embed_model,\n",
        "                    breakpoint_percentile_threshold=90,\n",
        "                )\n",
        "            except Exception:\n",
        "                parser = SentenceSplitter.from_defaults(chunk_size=120, chunk_overlap=20)\n",
        "            nodes = parser.get_nodes_from_documents(docs)\n",
        "            return nodes, nodes\n",
        "\n",
        "        if method in {\"1.4_query_aware_dynamic_chunking\", \"1.6_sentence_level_indexing\"}:\n",
        "            window = 2 if method == \"1.4_query_aware_dynamic_chunking\" else 1\n",
        "            parser = SentenceWindowNodeParser.from_defaults(window_size=window)\n",
        "            nodes = parser.get_nodes_from_documents(docs)\n",
        "            return nodes, nodes\n",
        "\n",
        "        if method == \"1.2_sliding_window_overlap\":\n",
        "            parser = TokenTextSplitter(chunk_size=120, chunk_overlap=40)\n",
        "            nodes = parser.get_nodes_from_documents(docs)\n",
        "            return nodes, nodes\n",
        "\n",
        "        chunk_size = int(self.runtime_config.get(\"chunk_size\", 120))\n",
        "        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "        nodes = parser.get_nodes_from_documents(docs)\n",
        "        return nodes, nodes\n",
        "\n",
        "    def _build_sparse_structures(self, texts: Sequence[str]) -> None:\n",
        "        tokenized = [tokenize(t) for t in texts]\n",
        "        self.term_freqs = [Counter(tokens) for tokens in tokenized]\n",
        "        for idx, tokens in enumerate(tokenized):\n",
        "            for token in set(tokens):\n",
        "                self.inverted[token].add(idx)\n",
        "\n",
        "    def _fit_clusters(self, k: int = 6) -> None:\n",
        "        if self.node_embeddings is None or len(self.node_embeddings) == 0:\n",
        "            return\n",
        "        n_samples = len(self.node_embeddings)\n",
        "        n_clusters = max(2, min(k, n_samples))\n",
        "        model = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
        "        self.cluster_labels = model.fit_predict(self.node_embeddings)\n",
        "        self.cluster_centers = model.cluster_centers_.astype(np.float32)\n",
        "\n",
        "    def _fit_semantic_hashing(self, bits: int = 32) -> None:\n",
        "        if self.node_embeddings is None:\n",
        "            return\n",
        "        rng = np.random.default_rng(42)\n",
        "        self.hash_planes = rng.normal(0.0, 1.0, size=(self.node_embeddings.shape[1], bits)).astype(np.float32)\n",
        "        projections = np.dot(self.node_embeddings, self.hash_planes)\n",
        "        self.hash_codes = (projections > 0).astype(np.int8)\n",
        "\n",
        "    def _summarize_text(self, text: str) -> str:\n",
        "        sents = split_sentences(text)\n",
        "        if not sents:\n",
        "            return text[:160]\n",
        "        if len(sents) == 1:\n",
        "            return sents[0]\n",
        "        return f\"{sents[0]} {sents[-1]}\"\n",
        "\n",
        "    def _build_entity_index_graph(self) -> None:\n",
        "        for idx, node in enumerate(self.index_nodes):\n",
        "            for entity in extract_entities(node_text(node)):\n",
        "                key = entity.lower()\n",
        "                self.entity_index[key].add(idx)\n",
        "\n",
        "        if self.method_id == \"5.1_knowledge_graph_index\" and nx is not None:\n",
        "            graph = nx.Graph()\n",
        "            for idx, node in enumerate(self.index_nodes):\n",
        "                cnode = f\"node::{idx}\"\n",
        "                graph.add_node(cnode, node_type=\"chunk\")\n",
        "                ents = extract_entities(node_text(node))\n",
        "                for ent in ents:\n",
        "                    enode = f\"entity::{ent.lower()}\"\n",
        "                    graph.add_node(enode, node_type=\"entity\")\n",
        "                    graph.add_edge(cnode, enode)\n",
        "            self.entity_graph = graph\n",
        "\n",
        "    def _build_semantic_graph(self, k: int = 4) -> None:\n",
        "        if self.node_embeddings is None:\n",
        "            return\n",
        "        sims = np.dot(self.node_embeddings, self.node_embeddings.T)\n",
        "        np.fill_diagonal(sims, -np.inf)\n",
        "        for i in range(len(self.index_nodes)):\n",
        "            neighbors = np.argsort(-sims[i])[:k]\n",
        "            for n in neighbors:\n",
        "                self.semantic_graph[i].add(int(n))\n",
        "                self.semantic_graph[int(n)].add(i)\n",
        "\n",
        "    def _build_outlier_mask(self) -> None:\n",
        "        if self.node_embeddings is None or len(self.node_embeddings) < 8:\n",
        "            self.outlier_mask = np.ones(len(self.index_nodes), dtype=bool)\n",
        "            return\n",
        "        model = IsolationForest(contamination=0.08, random_state=42)\n",
        "        labels = model.fit_predict(self.node_embeddings)\n",
        "        self.outlier_mask = labels > 0\n",
        "\n",
        "    def _build_drift_weights(self) -> None:\n",
        "        if self.node_embeddings is None:\n",
        "            return\n",
        "        center = np.mean(self.node_embeddings, axis=0)\n",
        "        dists = np.linalg.norm(self.node_embeddings - center, axis=1)\n",
        "        if np.max(dists) <= 1e-6:\n",
        "            self.drift_weights = np.ones_like(dists)\n",
        "            return\n",
        "        norm = dists / (np.max(dists) + 1e-6)\n",
        "        self.drift_weights = (1.0 - 0.5 * norm).astype(np.float32)\n",
        "\n",
        "    def _build_causal_graph(self) -> None:\n",
        "        markers = [\"because\", \"led to\", \"resulted in\", \"therefore\", \"caused\"]\n",
        "        marker_to_nodes: Dict[str, List[int]] = defaultdict(list)\n",
        "        for idx, node in enumerate(self.index_nodes):\n",
        "            text = node_text(node).lower()\n",
        "            for marker in markers:\n",
        "                if marker in text:\n",
        "                    marker_to_nodes[marker].append(idx)\n",
        "        for node_ids in marker_to_nodes.values():\n",
        "            for i in node_ids:\n",
        "                for j in node_ids:\n",
        "                    if i != j:\n",
        "                        self.causal_graph[i].add(j)\n",
        "\n",
        "    def _estimate_variance(self, text: str) -> float:\n",
        "        toks = tokenize(text)\n",
        "        if not toks:\n",
        "            return 1.0\n",
        "        uniq = len(set(toks))\n",
        "        diversity = uniq / len(toks)\n",
        "        length_penalty = 1.0 / math.sqrt(len(toks))\n",
        "        return float(0.6 * (1.0 - diversity) + 0.4 * length_penalty)\n",
        "\n",
        "    def _build_method_specific_structures(self, texts: Sequence[str]) -> None:\n",
        "        if self.method_id in {\n",
        "            \"2.2_approximate_nearest_neighbor_index\",\n",
        "            \"2.4_hybrid_dense_partition_index\",\n",
        "            \"6.1_topic_partition_index\",\n",
        "        }:\n",
        "            self._fit_clusters()\n",
        "\n",
        "        if self.method_id == \"6.2_semantic_hashing_index\":\n",
        "            self._fit_semantic_hashing()\n",
        "\n",
        "        if self.method_id == \"4.2_summary_augmented_index\":\n",
        "            self.summaries = [self._summarize_text(t) for t in texts]\n",
        "            try:\n",
        "                raw = Settings.embed_model.get_text_embedding_batch(self.summaries)\n",
        "                self.summary_embeddings = np.asarray(raw, dtype=np.float32)\n",
        "            except Exception:\n",
        "                self.summary_embeddings = None\n",
        "\n",
        "        if self.method_id in {\n",
        "            \"5.1_knowledge_graph_index\",\n",
        "            \"5.2_entity_centric_index\",\n",
        "            \"5.4_multi_hop_retrieval_graph\",\n",
        "        }:\n",
        "            self._build_entity_index_graph()\n",
        "\n",
        "        if self.method_id in {\"5.3_semantic_similarity_graph\", \"5.4_multi_hop_retrieval_graph\"}:\n",
        "            self._build_semantic_graph(k=4)\n",
        "\n",
        "        if self.method_id == \"8.2_outlier_aware_vector_index\":\n",
        "            self._build_outlier_mask()\n",
        "\n",
        "        if self.method_id == \"8.3_drift_aware_index\":\n",
        "            self._build_drift_weights()\n",
        "\n",
        "        if self.method_id == \"12.1_causal_graph_index\":\n",
        "            self._build_causal_graph()\n",
        "\n",
        "    def build(self, documents: Sequence[Dict[str, Any]]) -> None:\n",
        "        self.documents = [dict(doc) for doc in documents]\n",
        "\n",
        "        self.embed_model, self.embed_model_v2, self.embed_source = _configure_embed_models()\n",
        "\n",
        "        llama_docs = self._to_documents(self.documents)\n",
        "        self.all_nodes, self.index_nodes = self._build_nodes(llama_docs)\n",
        "\n",
        "        if self.method_id == \"1.5_small_to_big_hierarchical_chunking\":\n",
        "            self.storage_context = StorageContext.from_defaults()\n",
        "            self.storage_context.docstore.add_documents(self.all_nodes)\n",
        "            self.vector_index = VectorStoreIndex(self.index_nodes, storage_context=self.storage_context)\n",
        "        else:\n",
        "            self.vector_index = VectorStoreIndex(self.index_nodes)\n",
        "\n",
        "        self.dense_retriever = self.vector_index.as_retriever(similarity_top_k=self.candidate_k)\n",
        "\n",
        "        if BM25Retriever is not None:\n",
        "            try:\n",
        "                bm25_top_k = min(self.candidate_k, max(1, len(self.index_nodes)))\n",
        "                self.bm25_retriever = BM25Retriever.from_defaults(nodes=self.index_nodes, similarity_top_k=bm25_top_k)\n",
        "            except Exception:\n",
        "                self.bm25_retriever = None\n",
        "\n",
        "        self.node_lookup = {node.node_id: node for node in self.all_nodes}\n",
        "        self.node_id_to_idx = {node.node_id: idx for idx, node in enumerate(self.index_nodes)}\n",
        "\n",
        "        texts = [node_text(node) for node in self.index_nodes]\n",
        "        self._build_sparse_structures(texts)\n",
        "\n",
        "        try:\n",
        "            raw_embeddings = Settings.embed_model.get_text_embedding_batch(texts)\n",
        "            self.node_embeddings = np.asarray(raw_embeddings, dtype=np.float32)\n",
        "        except Exception:\n",
        "            self.node_embeddings = np.zeros((len(texts), 384), dtype=np.float32)\n",
        "\n",
        "        if self.method_id == \"11.3_continual_re_embedding_index\":\n",
        "            if self.embed_model_v2 is not None:\n",
        "                try:\n",
        "                    raw_v2 = self.embed_model_v2.get_text_embedding_batch(texts)\n",
        "                    self.node_embeddings_v2 = np.asarray(raw_v2, dtype=np.float32)\n",
        "                except Exception:\n",
        "                    self.node_embeddings_v2 = None\n",
        "            if self.node_embeddings_v2 is None:\n",
        "                self.node_embeddings_v2 = np.roll(self.node_embeddings, shift=1, axis=1)\n",
        "\n",
        "        self.trust_scores = np.array([\n",
        "            safe_float((node.metadata or {}).get(\"trust_score\", 0.8), 0.8) for node in self.index_nodes\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        today = date.today()\n",
        "        age_days: List[float] = []\n",
        "        for node in self.index_nodes:\n",
        "            raw_ts = (node.metadata or {}).get(\"timestamp\")\n",
        "            ts_str: Optional[str]\n",
        "            if isinstance(raw_ts, date):\n",
        "                ts_str = raw_ts.isoformat()\n",
        "            elif raw_ts is None:\n",
        "                ts_str = None\n",
        "            else:\n",
        "                ts_str = str(raw_ts)\n",
        "\n",
        "            parsed_ts = parse_date(ts_str)\n",
        "            if parsed_ts is None:\n",
        "                age_days.append(3650.0)\n",
        "            else:\n",
        "                age_days.append(float((today - parsed_ts).days))\n",
        "\n",
        "        self.age_days = np.array(age_days, dtype=np.float32)\n",
        "\n",
        "        variance_scores = np.array([self._estimate_variance(t) for t in texts], dtype=np.float32)\n",
        "        self.variance_scores = variance_scores\n",
        "        self.confidence_scores = np.exp(-2.0 * variance_scores).astype(np.float32)\n",
        "\n",
        "        self._build_method_specific_structures(texts)\n",
        "\n",
        "    def _query_embedding(self, query: str, use_v2: bool = False) -> np.ndarray:\n",
        "        if use_v2 and self.embed_model_v2 is not None:\n",
        "            try:\n",
        "                return np.asarray(self.embed_model_v2.get_query_embedding(query), dtype=np.float32)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return np.asarray(Settings.embed_model.get_query_embedding(query), dtype=np.float32)\n",
        "\n",
        "    def _dense_scores(self, query: str, use_v2: bool = False) -> np.ndarray:\n",
        "        n = len(self.index_nodes)\n",
        "        if n == 0:\n",
        "            return np.array([], dtype=np.float32)\n",
        "\n",
        "        q = self._query_embedding(query, use_v2=use_v2)\n",
        "        emb = self.node_embeddings_v2 if (use_v2 and self.node_embeddings_v2 is not None) else self.node_embeddings\n",
        "        if emb is None:\n",
        "            return np.zeros(n, dtype=np.float32)\n",
        "\n",
        "        dot_scores = np.dot(emb, q).astype(np.float32)\n",
        "\n",
        "        if self.dense_retriever is None:\n",
        "            return dot_scores\n",
        "\n",
        "        retriever_scores = np.full(n, -np.inf, dtype=np.float32)\n",
        "        try:\n",
        "            hits = self.dense_retriever.retrieve(query)\n",
        "        except Exception:\n",
        "            hits = []\n",
        "\n",
        "        for hit in hits:\n",
        "            idx = self.node_id_to_idx.get(hit.node.node_id)\n",
        "            if idx is None:\n",
        "                continue\n",
        "            value = safe_float(hit.score, 0.0)\n",
        "            if not np.isfinite(retriever_scores[idx]) or value > retriever_scores[idx]:\n",
        "                retriever_scores[idx] = value\n",
        "\n",
        "        return np.where(np.isfinite(retriever_scores), retriever_scores, dot_scores)\n",
        "\n",
        "    def _bm25_scores(self, query: str) -> np.ndarray:\n",
        "        n = len(self.index_nodes)\n",
        "        if n == 0:\n",
        "            return np.array([], dtype=np.float32)\n",
        "        if self.bm25_retriever is None:\n",
        "            return self._inverted_scores(query)\n",
        "\n",
        "        scores = np.zeros(n, dtype=np.float32)\n",
        "        try:\n",
        "            hits = self.bm25_retriever.retrieve(query)\n",
        "        except Exception:\n",
        "            hits = []\n",
        "\n",
        "        for hit in hits:\n",
        "            idx = self.node_id_to_idx.get(hit.node.node_id)\n",
        "            if idx is None:\n",
        "                continue\n",
        "            scores[idx] = max(scores[idx], safe_float(hit.score, 0.0))\n",
        "        return scores\n",
        "\n",
        "    def _inverted_scores(self, query: str) -> np.ndarray:\n",
        "        q_tokens = tokenize(query)\n",
        "        n_docs = len(self.index_nodes)\n",
        "        scores = np.zeros(n_docs, dtype=np.float32)\n",
        "        for token in q_tokens:\n",
        "            postings = self.inverted.get(token)\n",
        "            if not postings:\n",
        "                continue\n",
        "            df = len(postings)\n",
        "            idf = math.log((n_docs + 1.0) / (df + 1.0)) + 1.0\n",
        "            for idx in postings:\n",
        "                tf = self.term_freqs[idx].get(token, 0)\n",
        "                scores[idx] += tf * idf\n",
        "        return scores\n",
        "\n",
        "    def _cluster_candidate_mask(self, query: str, top_clusters: int = 2) -> np.ndarray:\n",
        "        if self.cluster_centers is None or self.cluster_labels is None:\n",
        "            return np.ones(len(self.index_nodes), dtype=bool)\n",
        "        q = self._query_embedding(query)\n",
        "        centroid_scores = np.dot(self.cluster_centers, q)\n",
        "        keep_clusters = set(np.argsort(-centroid_scores)[:top_clusters].tolist())\n",
        "        return np.array([int(label) in keep_clusters for label in self.cluster_labels], dtype=bool)\n",
        "\n",
        "    def _semantic_hash_candidates(self, query: str, threshold: int = 8) -> np.ndarray:\n",
        "        if self.hash_planes is None or self.hash_codes is None:\n",
        "            return np.ones(len(self.index_nodes), dtype=bool)\n",
        "        q = self._query_embedding(query)\n",
        "        q_code = (np.dot(q, self.hash_planes) > 0).astype(np.int8)\n",
        "        distances = np.sum(np.abs(self.hash_codes - q_code), axis=1)\n",
        "        if np.min(distances) > threshold:\n",
        "            keep = np.argsort(distances)[: min(20, len(distances))]\n",
        "            mask = np.zeros(len(distances), dtype=bool)\n",
        "            mask[keep] = True\n",
        "            return mask\n",
        "        return distances <= threshold\n",
        "\n",
        "    def _score_query_aware(self, query: str) -> np.ndarray:\n",
        "        scores = self._dense_scores(query)\n",
        "        q_tokens = set(tokenize(query))\n",
        "        for idx, node in enumerate(self.index_nodes):\n",
        "            text = node_text(node)\n",
        "            overlap = len(q_tokens & set(tokenize(text)))\n",
        "            scores[idx] += 0.2 * overlap\n",
        "            window_text = str((node.metadata or {}).get(\"window\", \"\"))\n",
        "            if window_text:\n",
        "                window_overlap = len(q_tokens & set(tokenize(window_text)))\n",
        "                scores[idx] += 0.08 * window_overlap\n",
        "        return scores\n",
        "\n",
        "    def _apply_graph_expansion(self, scores: np.ndarray, hops: int = 1, decay: float = 0.2) -> np.ndarray:\n",
        "        if not self.semantic_graph:\n",
        "            return scores\n",
        "        expanded = scores.copy()\n",
        "        seeds = np.argsort(-scores)[: min(6, len(scores))]\n",
        "        frontier = {(int(seed), 0) for seed in seeds}\n",
        "        visited = set(int(seed) for seed in seeds)\n",
        "\n",
        "        while frontier:\n",
        "            node, depth = frontier.pop()\n",
        "            if depth >= hops:\n",
        "                continue\n",
        "            for nbr in self.semantic_graph.get(node, set()):\n",
        "                expanded[nbr] += max(0.0, scores[node]) * (decay ** (depth + 1))\n",
        "                if nbr not in visited:\n",
        "                    visited.add(nbr)\n",
        "                    frontier.add((nbr, depth + 1))\n",
        "        return expanded\n",
        "\n",
        "    def _apply_entity_boost(self, query: str, base_scores: np.ndarray, weight: float = 0.35) -> np.ndarray:\n",
        "        entities = [e.lower() for e in extract_entities(query)]\n",
        "        if not entities:\n",
        "            return base_scores\n",
        "        scores = base_scores.copy()\n",
        "        for ent in entities:\n",
        "            for idx in self.entity_index.get(ent, set()):\n",
        "                scores[idx] += weight\n",
        "        return scores\n",
        "\n",
        "    def _apply_scene_aware_media_adjustment(self, query: str, scores: np.ndarray) -> np.ndarray:\n",
        "        q_tokens = set(tokenize(query))\n",
        "        if not q_tokens:\n",
        "            return scores\n",
        "\n",
        "        scene_bonus = np.array(\n",
        "            [\n",
        "                0.2\n",
        "                if any(tok in str((node.metadata or {}).get(\"scene\", \"\")).lower() for tok in q_tokens)\n",
        "                else 0.0\n",
        "                for node in self.index_nodes\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        return scores + scene_bonus\n",
        "\n",
        "    def _apply_feedback_bias(self, scores: np.ndarray) -> np.ndarray:\n",
        "        bias = np.array([self.feedback_bias.get(node.node_id, 0.0) for node in self.index_nodes], dtype=np.float32)\n",
        "        return scores + bias\n",
        "\n",
        "    def _apply_continual_reembedding_fusion(self, query: str, scores: np.ndarray) -> np.ndarray:\n",
        "        v1 = normalize_scores(scores)\n",
        "        v2 = normalize_scores(self._dense_scores(query, use_v2=True))\n",
        "        return 0.35 * v1 + 0.65 * v2\n",
        "\n",
        "    def _apply_causal_graph_boost(self, query: str, scores: np.ndarray) -> np.ndarray:\n",
        "        query_lower = query.lower()\n",
        "        if not any(marker in query_lower for marker in self._CAUSAL_QUERY_MARKERS):\n",
        "            return scores\n",
        "\n",
        "        boosted = scores.copy()\n",
        "        for seed in np.argsort(-scores)[: min(5, len(scores))]:\n",
        "            for nbr in self.causal_graph.get(int(seed), set()):\n",
        "                boosted[nbr] += 0.2\n",
        "        return boosted\n",
        "\n",
        "    def _apply_explanation_bonus(self, query: str, scores: np.ndarray) -> np.ndarray:\n",
        "        q_lower = query.lower()\n",
        "        if not any(token in q_lower for token in self._EXPLANATION_QUERY_MARKERS):\n",
        "            return scores\n",
        "\n",
        "        bonus = np.array(\n",
        "            [\n",
        "                0.15 if any(marker in node_text(node).lower() for marker in self._EXPLANATION_TEXT_MARKERS) else 0.0\n",
        "                for node in self.index_nodes\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        return scores + bonus\n",
        "\n",
        "    def _apply_plan_bonus(self, query: str, scores: np.ndarray) -> np.ndarray:\n",
        "        q_lower = query.lower()\n",
        "        if not any(token in q_lower for token in self._PLAN_QUERY_MARKERS):\n",
        "            return scores\n",
        "\n",
        "        bonus = np.array(\n",
        "            [\n",
        "                0.18 if any(marker in node_text(node).lower() for marker in self._PLAN_TEXT_MARKERS) else 0.0\n",
        "                for node in self.index_nodes\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        return scores + bonus\n",
        "\n",
        "    def _scores_for_method(self, query: str) -> np.ndarray:\n",
        "        m = self.method_id\n",
        "\n",
        "        if m == \"3.1_inverted_index\":\n",
        "            return self._inverted_scores(query)\n",
        "\n",
        "        if m == \"3.2_sparse_dense_fusion_index\":\n",
        "            dense = normalize_scores(self._dense_scores(query))\n",
        "            sparse = normalize_scores(self._bm25_scores(query))\n",
        "            return 0.55 * dense + 0.45 * sparse\n",
        "\n",
        "        if m == \"3.3_weighted_term_index\":\n",
        "            return self._bm25_scores(query)\n",
        "\n",
        "        scores = self._dense_scores(query)\n",
        "\n",
        "        if m == \"1.4_query_aware_dynamic_chunking\":\n",
        "            scores = self._score_query_aware(query)\n",
        "\n",
        "        elif m == \"2.2_approximate_nearest_neighbor_index\":\n",
        "            mask = self._cluster_candidate_mask(query, top_clusters=2)\n",
        "            scores = np.where(mask, scores, -np.inf)\n",
        "\n",
        "        elif m == \"2.3_quantized_vector_index\":\n",
        "            q = self._query_embedding(query).astype(np.float16)\n",
        "            emb = self.node_embeddings.astype(np.float16) if self.node_embeddings is not None else None\n",
        "            if emb is not None:\n",
        "                scores = np.dot(emb, q).astype(np.float32)\n",
        "\n",
        "        elif m == \"2.4_hybrid_dense_partition_index\":\n",
        "            mask = self._cluster_candidate_mask(query, top_clusters=3)\n",
        "            masked_scores = np.where(mask, scores, -np.inf)\n",
        "            cluster_prior = np.zeros_like(scores)\n",
        "            if self.cluster_labels is not None and self.cluster_centers is not None:\n",
        "                q = self._query_embedding(query)\n",
        "                centroid_scores = np.dot(self.cluster_centers, q)\n",
        "                norm_centroids = normalize_scores(centroid_scores)\n",
        "                for i, label in enumerate(self.cluster_labels):\n",
        "                    cluster_prior[i] = norm_centroids[int(label)]\n",
        "            scores = 0.7 * normalize_scores(masked_scores) + 0.3 * cluster_prior\n",
        "\n",
        "        elif m == \"4.1_tree_structured_index\":\n",
        "            depth_boost = np.array([\n",
        "                1.0 / (1.0 + safe_float((node.metadata or {}).get(\"depth\", 0), 0.0))\n",
        "                for node in self.index_nodes\n",
        "            ], dtype=np.float32)\n",
        "            scores = scores * (0.7 + 0.3 * depth_boost)\n",
        "\n",
        "        elif m == \"4.2_summary_augmented_index\":\n",
        "            if self.summary_embeddings is not None:\n",
        "                q = self._query_embedding(query)\n",
        "                summary_scores = np.dot(self.summary_embeddings, q)\n",
        "                scores = 0.4 * normalize_scores(scores) + 0.6 * normalize_scores(summary_scores)\n",
        "\n",
        "        elif m == \"4.3_structural_aware_index\":\n",
        "            q_tokens = set(tokenize(query))\n",
        "            bonuses = []\n",
        "            for node in self.index_nodes:\n",
        "                heading = str((node.metadata or {}).get(\"heading\", \"\")).lower()\n",
        "                bonus = sum(0.15 for token in q_tokens if token in heading)\n",
        "                bonuses.append(bonus)\n",
        "            scores = scores + np.array(bonuses, dtype=np.float32)\n",
        "\n",
        "        elif m == \"5.1_knowledge_graph_index\":\n",
        "            scores = self._apply_entity_boost(query, scores, weight=0.5)\n",
        "\n",
        "        elif m == \"5.2_entity_centric_index\":\n",
        "            entity_scores = self._apply_entity_boost(query, np.zeros_like(scores), weight=1.0)\n",
        "            if float(np.max(entity_scores)) > 0:\n",
        "                scores = 0.25 * normalize_scores(scores) + 0.75 * normalize_scores(entity_scores)\n",
        "\n",
        "        elif m == \"5.3_semantic_similarity_graph\":\n",
        "            scores = self._apply_graph_expansion(scores, hops=1, decay=0.25)\n",
        "\n",
        "        elif m == \"5.4_multi_hop_retrieval_graph\":\n",
        "            scores = self._apply_entity_boost(query, scores, weight=0.25)\n",
        "            scores = self._apply_graph_expansion(scores, hops=2, decay=0.25)\n",
        "\n",
        "        elif m == \"6.1_topic_partition_index\":\n",
        "            mask = self._cluster_candidate_mask(query, top_clusters=1)\n",
        "            scores = np.where(mask, scores, -np.inf)\n",
        "\n",
        "        elif m == \"6.2_semantic_hashing_index\":\n",
        "            mask = self._semantic_hash_candidates(query)\n",
        "            scores = np.where(mask, scores, -np.inf)\n",
        "\n",
        "        elif m == \"7.1_time_decayed_index\":\n",
        "            if self.age_days is not None:\n",
        "                decay = np.exp(-self.age_days / 365.0)\n",
        "                scores = scores * decay\n",
        "\n",
        "        elif m == \"7.2_sliding_temporal_window_index\":\n",
        "            if self.age_days is not None:\n",
        "                recent = self.age_days <= 180\n",
        "                weights = np.where(recent, 1.1, 0.8)\n",
        "                scores = scores * weights\n",
        "\n",
        "        elif m == \"7.3_real_time_streaming_index\":\n",
        "            if self.age_days is not None:\n",
        "                recency = normalize_scores(-self.age_days)\n",
        "                scores = 0.75 * normalize_scores(scores) + 0.25 * recency\n",
        "\n",
        "        elif m == \"8.1_trust_weighted_index\":\n",
        "            if self.trust_scores is not None:\n",
        "                scores = scores * self.trust_scores\n",
        "\n",
        "        elif m == \"8.2_outlier_aware_vector_index\":\n",
        "            if self.outlier_mask is not None:\n",
        "                scores = np.where(self.outlier_mask, scores, -np.inf)\n",
        "\n",
        "        elif m == \"8.3_drift_aware_index\":\n",
        "            if self.drift_weights is not None:\n",
        "                scores = scores * self.drift_weights\n",
        "\n",
        "        elif m == \"9.1_distributional_embedding_index\":\n",
        "            if self.variance_scores is not None:\n",
        "                scores = scores / (1.0 + self.variance_scores)\n",
        "\n",
        "        elif m == \"9.2_confidence_propagating_index\":\n",
        "            if self.confidence_scores is not None:\n",
        "                scores = scores * self.confidence_scores\n",
        "\n",
        "        elif m == \"10.3_scene_aware_media_index\":\n",
        "            scores = self._apply_scene_aware_media_adjustment(query, scores)\n",
        "\n",
        "        elif m == \"11.2_feedback_driven_index\":\n",
        "            scores = self._apply_feedback_bias(scores)\n",
        "\n",
        "        elif m == \"11.3_continual_re_embedding_index\":\n",
        "            scores = self._apply_continual_reembedding_fusion(query, scores)\n",
        "\n",
        "        elif m == \"12.1_causal_graph_index\":\n",
        "            scores = self._apply_causal_graph_boost(query, scores)\n",
        "\n",
        "        elif m == \"12.2_explanation_aware_index\":\n",
        "            scores = self._apply_explanation_bonus(query, scores)\n",
        "\n",
        "        elif m == \"12.3_plan_oriented_index\":\n",
        "            scores = self._apply_plan_bonus(query, scores)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5) -> List[ScoredNode]:\n",
        "        if not self.index_nodes:\n",
        "            return []\n",
        "\n",
        "        scores = self._scores_for_method(query)\n",
        "        valid = np.isfinite(scores)\n",
        "        if not valid.any():\n",
        "            return []\n",
        "\n",
        "        order = np.argsort(-scores)\n",
        "        raw_rows: List[ScoredNode] = []\n",
        "        for idx in order:\n",
        "            idx = int(idx)\n",
        "            if not np.isfinite(scores[idx]):\n",
        "                continue\n",
        "            node = self.index_nodes[idx]\n",
        "            meta = dict(node.metadata or {})\n",
        "            doc_id = str(meta.get(\"doc_id\") or meta.get(\"id\") or node.node_id)\n",
        "            raw_rows.append(\n",
        "                ScoredNode(\n",
        "                    node_id=node.node_id,\n",
        "                    doc_id=doc_id,\n",
        "                    score=float(scores[idx]),\n",
        "                    text=node_text(node),\n",
        "                    metadata=meta,\n",
        "                )\n",
        "            )\n",
        "            if len(raw_rows) >= max(top_k * 4, 16):\n",
        "                break\n",
        "\n",
        "        if self.method_id == \"1.5_small_to_big_hierarchical_chunking\":\n",
        "            merged: Dict[str, ScoredNode] = {}\n",
        "            for row in raw_rows:\n",
        "                parent_id = self.parent_map.get(row.node_id)\n",
        "                if not parent_id:\n",
        "                    continue\n",
        "                parent_node = self.node_lookup.get(parent_id)\n",
        "                if parent_node is None:\n",
        "                    continue\n",
        "                parent_meta = dict(parent_node.metadata or {})\n",
        "                parent_doc_id = str(parent_meta.get(\"doc_id\") or parent_meta.get(\"id\") or parent_id)\n",
        "                existing = merged.get(parent_id)\n",
        "                candidate = ScoredNode(\n",
        "                    node_id=parent_id,\n",
        "                    doc_id=parent_doc_id,\n",
        "                    score=row.score,\n",
        "                    text=node_text(parent_node),\n",
        "                    metadata=parent_meta,\n",
        "                )\n",
        "                if existing is None or candidate.score > existing.score:\n",
        "                    merged[parent_id] = candidate\n",
        "            raw_rows = sorted(merged.values(), key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        if self.method_id == \"10.2_cross_modal_linked_index\":\n",
        "            by_asset: Dict[str, ScoredNode] = {}\n",
        "            for row in raw_rows:\n",
        "                asset_id = str(row.metadata.get(\"asset_id\") or \"\")\n",
        "                if not asset_id:\n",
        "                    continue\n",
        "                best = by_asset.get(asset_id)\n",
        "                if best is None or row.score > best.score:\n",
        "                    by_asset[asset_id] = row\n",
        "\n",
        "            expanded = list(raw_rows)\n",
        "            seen_node_ids = {row.node_id for row in raw_rows}\n",
        "            for asset_id, best_row in by_asset.items():\n",
        "                for node in self.index_nodes:\n",
        "                    meta = dict(node.metadata or {})\n",
        "                    if str(meta.get(\"asset_id\") or \"\") != asset_id:\n",
        "                        continue\n",
        "                    if node.node_id in seen_node_ids:\n",
        "                        continue\n",
        "                    expanded.append(\n",
        "                        ScoredNode(\n",
        "                            node_id=node.node_id,\n",
        "                            doc_id=str(meta.get(\"doc_id\") or meta.get(\"id\") or node.node_id),\n",
        "                            score=float(best_row.score * 0.92),\n",
        "                            text=node_text(node),\n",
        "                            metadata=meta,\n",
        "                        )\n",
        "                    )\n",
        "                    seen_node_ids.add(node.node_id)\n",
        "            raw_rows = sorted(expanded, key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        final_rows: List[ScoredNode] = []\n",
        "        seen_docs = set()\n",
        "        for row in raw_rows:\n",
        "            if row.doc_id in seen_docs:\n",
        "                continue\n",
        "            seen_docs.add(row.doc_id)\n",
        "            final_rows.append(row)\n",
        "            if len(final_rows) >= top_k:\n",
        "                break\n",
        "        return final_rows\n",
        "\n",
        "    def evaluate(self, queries: Sequence[Dict[str, Any]], top_k: int = 5) -> Dict[str, Any]:\n",
        "        per_query: List[Dict[str, Any]] = []\n",
        "\n",
        "        p_scores = []\n",
        "        r_scores = []\n",
        "        mrr_scores = []\n",
        "        ndcg_scores = []\n",
        "\n",
        "        for item in queries:\n",
        "            query = str(item.get(\"query\", \"\"))\n",
        "            relevant = set(str(x) for x in item.get(\"relevant_ids\", []))\n",
        "            results = self.search(query, top_k=top_k)\n",
        "            ranked_ids = [row.doc_id for row in results]\n",
        "\n",
        "            hits = [doc_id for doc_id in ranked_ids if doc_id in relevant]\n",
        "            precision = len(hits) / max(1, top_k)\n",
        "            recall = len(hits) / max(1, len(relevant))\n",
        "\n",
        "            rr = 0.0\n",
        "            for idx, doc_id in enumerate(ranked_ids, start=1):\n",
        "                if doc_id in relevant:\n",
        "                    rr = 1.0 / idx\n",
        "                    break\n",
        "\n",
        "            dcg = 0.0\n",
        "            for idx, doc_id in enumerate(ranked_ids, start=1):\n",
        "                rel = 1.0 if doc_id in relevant else 0.0\n",
        "                dcg += rel / math.log2(idx + 1)\n",
        "            ideal_hits = min(len(relevant), top_k)\n",
        "            idcg = sum(1.0 / math.log2(i + 1) for i in range(1, ideal_hits + 1))\n",
        "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "            p_scores.append(precision)\n",
        "            r_scores.append(recall)\n",
        "            mrr_scores.append(rr)\n",
        "            ndcg_scores.append(ndcg)\n",
        "\n",
        "            per_query.append(\n",
        "                {\n",
        "                    \"query\": query,\n",
        "                    \"relevant_ids\": sorted(relevant),\n",
        "                    \"retrieved_ids\": ranked_ids,\n",
        "                    \"precision_at_k\": precision,\n",
        "                    \"recall_at_k\": recall,\n",
        "                    \"mrr\": rr,\n",
        "                    \"ndcg_at_k\": ndcg,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if self.method_id == \"11.2_feedback_driven_index\":\n",
        "                for row in results:\n",
        "                    if row.doc_id in relevant:\n",
        "                        self.feedback_bias[row.node_id] += 0.05\n",
        "\n",
        "        metrics = {\n",
        "            \"precision_at_k\": float(np.mean(p_scores) if p_scores else 0.0),\n",
        "            \"recall_at_k\": float(np.mean(r_scores) if r_scores else 0.0),\n",
        "            \"mrr\": float(np.mean(mrr_scores) if mrr_scores else 0.0),\n",
        "            \"ndcg_at_k\": float(np.mean(ndcg_scores) if ndcg_scores else 0.0),\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"method_id\": self.spec.method_id,\n",
        "            \"method_name\": self.spec.name,\n",
        "            \"category\": self.spec.category,\n",
        "            \"description\": self.spec.description,\n",
        "            \"embedding_source\": self.embed_source,\n",
        "            \"documents\": len(self.documents),\n",
        "            \"nodes\": len(self.index_nodes),\n",
        "            \"metrics\": metrics,\n",
        "            \"runtime_config\": self.runtime_config,\n",
        "            \"per_query\": per_query,\n",
        "            \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
        "        }\n",
        "\n",
        "\n",
        "# Backward-compatible alias for previous API name.\n",
        "IndexingMethodRunner = LlamaIndexMethodRunner\n",
        "\n",
        "\n",
        "def run_method(\n",
        "    method_id: str,\n",
        "    corpus_path: Optional[str | Path] = None,\n",
        "    queries_path: Optional[str | Path] = None,\n",
        "    output_dir: str | Path = \"results\",\n",
        "    top_k: int = 5,\n",
        ") -> Dict[str, Any]:\n",
        "    runner = LlamaIndexMethodRunner(method_id)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "    queries = load_queries(queries_path)\n",
        "    runner.build(corpus)\n",
        "    result = runner.evaluate(queries, top_k=top_k)\n",
        "\n",
        "    out_dir = Path(output_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_path = out_dir / f\"{method_slug(method_id)}.json\"\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "        json.dump(result, handle, indent=2)\n",
        "\n",
        "    result[\"result_path\"] = str(out_path)\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_all_methods(\n",
        "    corpus_path: Optional[str | Path] = None,\n",
        "    queries_path: Optional[str | Path] = None,\n",
        "    output_dir: str | Path = \"results\",\n",
        "    top_k: int = 5,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    outputs = []\n",
        "    for spec in METHOD_SPECS:\n",
        "        outputs.append(\n",
        "            run_method(\n",
        "                method_id=spec.method_id,\n",
        "                corpus_path=corpus_path,\n",
        "                queries_path=queries_path,\n",
        "                output_dir=output_dir,\n",
        "                top_k=top_k,\n",
        "            )\n",
        "        )\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "METHOD_ID = \"1.4_query_aware_dynamic_chunking\"\n",
        "TOP_K = 5\n",
        "\n",
        "result = run_method(\n",
        "    method_id=METHOD_ID,\n",
        "    corpus_path=corpus_path,\n",
        "    queries_path=queries_path,\n",
        "    output_dir=results_dir,\n",
        "    top_k=TOP_K,\n",
        ")\n",
        "\n",
        "result[\"metrics\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(result[\"per_query\"])\n",
        "df[[\"query\", \"precision_at_k\", \"recall_at_k\", \"mrr\", \"ndcg_at_k\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Saved:\", result[\"result_path\"])\n",
        "print(\"Embedding source:\", result.get(\"embedding_source\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}