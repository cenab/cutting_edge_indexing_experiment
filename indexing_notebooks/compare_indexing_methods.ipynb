{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Indexing Method Comparison\n",
        "\n",
        "This notebook loads all method result files from `results/`, builds a ranking table, and shows category-level summaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if not (repo_root / \"indexing_experiments\").exists():\n",
        "    repo_root = repo_root.parent\n",
        "\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "from indexing_experiments import METHOD_SPECS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_dir = repo_root / \"results\"\n",
        "rows = []\n",
        "for path in sorted(results_dir.glob(\"*.json\")):\n",
        "    payload = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    metrics = payload.get(\"metrics\", {})\n",
        "    rows.append(\n",
        "        {\n",
        "            \"method_id\": payload.get(\"method_id\"),\n",
        "            \"method_name\": payload.get(\"method_name\"),\n",
        "            \"category\": payload.get(\"category\"),\n",
        "            \"precision_at_k\": metrics.get(\"precision_at_k\", 0.0),\n",
        "            \"recall_at_k\": metrics.get(\"recall_at_k\", 0.0),\n",
        "            \"mrr\": metrics.get(\"mrr\", 0.0),\n",
        "            \"ndcg_at_k\": metrics.get(\"ndcg_at_k\", 0.0),\n",
        "            \"chunks\": payload.get(\"chunks\", 0),\n",
        "            \"result_file\": path.name,\n",
        "        }\n",
        "    )\n",
        "\n",
        "score_df = pd.DataFrame(rows)\n",
        "score_df.sort_values([\"mrr\", \"ndcg_at_k\"], ascending=False).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(score_df) == 0:\n",
        "    print(\"No results found in results/. Run method notebooks first.\")\n",
        "else:\n",
        "    category_summary = (\n",
        "        score_df.groupby(\"category\", as_index=False)[[\"precision_at_k\", \"recall_at_k\", \"mrr\", \"ndcg_at_k\"]]\n",
        "        .mean()\n",
        "        .sort_values(\"mrr\", ascending=False)\n",
        "    )\n",
        "    display(category_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected = {spec.method_id for spec in METHOD_SPECS}\n",
        "available = set(score_df[\"method_id\"].tolist()) if len(score_df) else set()\n",
        "missing = sorted(expected - available)\n",
        "\n",
        "print(f\"Expected methods: {len(expected)}\")\n",
        "print(f\"Available result files: {len(available)}\")\n",
        "print(\"Missing methods:\")\n",
        "missing\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}